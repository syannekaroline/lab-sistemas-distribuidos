# Trabalho de labrat√≥rio de Sistemas distribu√≠dos: Sockets em Linux

Este reposit√≥rio cont√©m a resolu√ß√£o do trabalho da disciplina de Sistemas distribu√≠dos, focado na implementa√ß√£o de um servidor de sockets em C capaz de lidar com m√∫ltiplas conex√µes e executar um servi√ßo de compila√ß√£o remota.


## Descri√ß√£o do Trabalho

* O objetivo deste trabalho √© evoluir um servidor de socket TCP b√°sico, que inicialmente s√≥ atende um cliente por vez, para uma aplica√ß√£o robusta capaz de gerenciar m√∫ltiplas requisi√ß√µes de clientes de forma concorrente.

* O projeto final consiste em um **servidor de compila√ß√£o** e um **cliente com interface gr√°fica (GUI)** para interagir com este servi√ßo.

---

## üöÄ Quest√µes Propostas

Abaixo est√£o as quest√µes do trabalho e as se√ß√µes para as respostas e implementa√ß√µes correspondentes.

### Quest√£o 1: Solu√ß√µes para M√∫ltiplos Clientes

> Quais as solu√ß√µes poss√≠veis para que o programa servidor possa atender a uma quantidade indeterminada de requisi√ß√µes de programas clientes? 

#### Resposta

Conforme [M. 2008 pp. 36, 68, 121‚Äì122], uma das mais simples t√©cnicas para implementar um servidor concorrente √© por meio de fun√ß√µes fork do unix, que √© capaz de criar processo-filho para atender m√∫ltiplos clientes ao mesmo tempo. Para isso, o servidor aceita conex√µes clientes e bifurca a requisi√ß√£o com a chamada da fun√ß√£o fork para criar uma c√≥pia de si pr√≥prio (Figura 2.13), deixando o processo-filho atender a requisi√ß√£o do cliente. Dessa forma o processo-pai espera novas conex√µes e cria novos processos-filho a cada nova requisi√ß√£o(Figura 2.14). Assim, √© capaz de atender a uma quantidade indeterminada de requisi√ß√µes de programas clientes.

 ![texto alternativo](https://private-user-images.githubusercontent.com/87232098/515417264-39d48879-2b87-4fff-b5da-03b5193830d5.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjM0MTg2MzcsIm5iZiI6MTc2MzQxODMzNywicGF0aCI6Ii84NzIzMjA5OC81MTU0MTcyNjQtMzlkNDg4NzktMmI4Ny00ZmZmLWI1ZGEtMDNiNTE5MzgzMGQ1LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTExMTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUxMTE3VDIyMjUzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWZiODAwODBkNmM5ZTgyNzIxMTIyZTE3NjUyZmZiYzM4MWFjZDkxOWY4YTM3N2I0NmE4MzE4ZjUzZmU0ZWI3NDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.r0Ellhvq1-brHLy29QGZf_PqqdbaR0fub6NPh5IFpYQ)
 
![texto alternativo](https://private-user-images.githubusercontent.com/87232098/515417677-0322bf10-06d4-4134-a8d7-57a34487c7f9.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjM0MTg1NDYsIm5iZiI6MTc2MzQxODI0NiwicGF0aCI6Ii84NzIzMjA5OC81MTU0MTc2NzctMDMyMmJmMTAtMDZkNC00MTM0LWE4ZDctNTdhMzQ0ODdjN2Y5LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTExMTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUxMTE3VDIyMjQwNlomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWIzMjI0YjQxNDk4MzFlODYwMWIyYWNhYTllZjIyMzI3NTVlNzUxOThlYmZlNmE3ZmE5Y2ZmYzY4ZjVjNzRmOGEmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.ea3Y0Arp53pM7I7wNYSTNnMCIXzSGkjp60xOK8Iki18)

Na pr√°tica, conforme a imagem abaixo, o processo servidor (Pai) opera em um loop infinito, aguardando conex√µes bloqueado na chamada accept. Quando uma conex√£o √© estabelecida, o servidor cria um processo-filho via fork(). O processo Pai imediatamente fecha sua c√≥pia do socket conectado e volta a esperar por novas conex√µes em accept. O processo Filho, por sua vez, fecha o socket de escuta (pois n√£o o usar√°), processa toda a comunica√ß√£o com o cliente atrav√©s do socket conectado e, ao finalizar, fecha o socket conectado e termina sua execu√ß√£o.

![texto alternativo](https://private-user-images.githubusercontent.com/87232098/515419686-9dbc58f1-d43c-4274-bdbc-ef5d991a4aee.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjM0MTg4MDEsIm5iZiI6MTc2MzQxODUwMSwicGF0aCI6Ii84NzIzMjA5OC81MTU0MTk2ODYtOWRiYzU4ZjEtZDQzYy00Mjc0LWJkYmMtZWY1ZDk5MWE0YWVlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTExMTclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUxMTE3VDIyMjgyMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTIyZTg4ZmJhZjg0MGFmY2U5ZDE1YzhkNmI4OGQ4OWJiMjIyMWRiOTY2MTY5ZmNlNWY2ZjljZmVlNzg5YzI4YjkmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.y3bGB0OmQpnJnUkMNmZeKsDnC0C4zoS4BQVTljoLLOg)

Al√©m da abordagem baseada em **processos**, outra solu√ß√£o utilizada para lidar com m√∫ltiplos clientes √© o uso de **threads**, especialmente atrav√©s de um **pool de threads**. Essa estrat√©gia resolve problemas de custo e efici√™ncia apresentados na t√©cnica que cria um processo-filho por cliente.

> ‚ÄúA bifurca√ß√£o (fork) √© cara. A mem√≥ria √© copiada do pai para o filho [‚Ä¶] e mesmo com otimiza√ß√µes como copy-on-write, a opera√ß√£o continua sendo pesada.‚Äù  
> ‚Äî Stevens et al., 2008

> ‚Äú√â exigido um IPC para passar as informa√ß√µes entre o pai e o filho [‚Ä¶]. Retornar informa√ß√µes do filho para o pai d√° mais trabalho.‚Äù  

As threads s√£o bem mais leves e compartilharem a mesma mem√≥ria global, o que facilita comunica√ß√£o entre elas (embora introduza a necessidade de sincroniza√ß√£o).

Al√©m disso, conforme o pr√≥prio Stevens destaca:

> ‚Äú√â mais r√°pido pr√©-bifurcar um pool de filhos do que criar um filho para cada cliente. Em um sistema que suporta threads, √© razo√°vel esperar um aumento de velocidade semelhante, por meio da cria√ß√£o de um pool de threads quando o servidor inicia, em vez de criar um novo thread para cada cliente.‚Äù

> ‚ÄúO projeto b√°sico desse servidor √© criar um pool de threads e ent√£o deixar cada thread chamar `accept`. Em vez de ter cada thread bloqueada na chamada de `accept`, utilizaremos um bloqueio de mutex que permite que somente um thread por vez chame `accept`.‚Äù

Essa √© exatamente a l√≥gica implementada na solu√ß√£o usando pool de threads.

**‚úî Como funciona a Solu√ß√£o baseada em Pool de Threads**

1. **Inicializa√ß√£o**
   - Cria socket TCP, `bind()` e `listen()`.
   - Cria um pool fixo de threads (`THREAD_POOL_SIZE`) ao iniciar.
   - Inicializa uma fila circular (`connection_queue`) protegida por `pthread_mutex_t` e `pthread_cond_t`.

2. **Papel da thread principal**
   - Loop em `accept()` aguardando conex√µes.
   - Para cada conex√£o aceita:
     - trava o mutex
     - insere o socket na fila circular (`queue_push`)
     - atualiza `queue_end = (queue_end + 1) % MAX_QUEUE`
     - sinaliza uma *worker* (`pthread_cond_signal`)
     - libera o mutex
   - A thread principal apenas distribui conex√µes ‚Äî n√£o processa o cliente.

3. **Fila Circular**
   - Usa `queue_start` e `queue_end` com operador `%` para circularidade.
   - Evita estouro de mem√≥ria e permite reaproveitamento do buffer fixo.
   - Acesso protegido por `pthread_mutex_t`.

4. **Threads Trabalhadoras (Workers)**
   - Cada worker executa:
     1. trava o mutex
     2. se a fila estiver vazia ‚Üí `pthread_cond_wait()`
     3. retira socket da fila
     4. libera o mutex
     5. processa cliente (leitura/resposta/fechamento)
     6. volta ao in√≠cio do loop
   - Threads s√£o reaproveitadas ‚Äî n√£o s√£o recriadas por cliente.

5. **Sincroniza√ß√£o**
   - `pthread_mutex_t` garante exclus√£o m√∫tua na fila.
   - `pthread_cond_t` evita *busy-waiting*; acorda workers quando h√° trabalho.

6. **Vantagens**
   - Evita criar thread por cliente ‚Üí menos overhead.
   - Reduz consumo de mem√≥ria e explos√£o de threads.
   - Melhor escalabilidade em cargas altas at√© o limite do hardware.
   - Menor lat√™ncia em faixas baixa/m√©dia de concorr√™ncia (conforme testes de estresse).

**Refer√™ncias**

M., S., W. Richard; Fenner, Bill; Rudoff, Andrew (2008). Programa√ß√£o de Rede Unix - Api para Soquetes de Rede. 

---

### Quest√£o 2: Implementa√ß√£o e Teste de Stress

> Implemente todas as solu√ß√µes determinadas e execute um teste de stress para verificar at√© quantas requisi√ß√µes o servidor/sistema operacional pode atender sem apresentar problemas.

#### Implementa√ß√µes

* **Solu√ß√£o 1 - Fork:** `[ /s1-fork/]`
* **Solu√ß√£o 2 ‚Äì Pool de Threads:** `[ /s2-thread-pool/ ]`

#### Resultados do Teste de Stress

**Solu√ß√£o 1 - fork**

O teste foi realizado em uma m√°quina Dell Inspiron 3501 com 12,0 GiB de RAM e Ubuntu 22.04.4 LTS . A metodologia consistiu em iniciar o servidor e, em outro terminal, executar um script stress_test.sh. Este script foi configurado para o host local (127.0.0.1) e porta 51482 , lan√ßando um n√∫mero vari√°vel de clientes (NUM_CLIENTS) em background (&) e aguardando todos terminarem com o comando wait. Os testes foram executados em etapas, aumentando o n√∫mero de clientes de 100 at√© 100.000. Os resultados mostraram que, embora tenham ocorrido erros de "Connection reset by peer" nos testes de 1.000 e 2.000 clientes, os testes com 50.000 e 100.000 clientes foram conclu√≠dos com sucesso. O teste de 100.000 clientes foi o √∫ltimo e mais longo da bateria, levando 6 minutos e 37,757 segundos para terminar, sendo esse o limite m√°ximo de clientes avaliado neste experimento, mesmo sem apresentar erros.

| N√∫mero de clientes | Erros observados (cliente) | Erros observados (servidor) | Dura√ß√£o Total (real) |
| :--- | :--- | :--- | :--- |
| 100 |  | | 0m1,069s |
| 500 |  | | 0m1,170s |
| 1000 | 240 vzs - ERROR reading from socket: Connection reset by peer |  | 2m0,428s |
| 2000 | 88 vzs - ERROR reading from socket: Connection reset by peer | | 2m2,848s |
| 4000 |  | | 0m3,000s |
| 8000 |  | | 0m4,558s |
| 12000 |  | | 0m8,331s |
| 50000 |  | | 2m4,123s |
| 100000 |  | | 6m37,757s |

Os testes foram realizados separadamente para cada solu√ß√£o. A seguir est√£o as tabelas, contexto do ambiente de hardware/software e an√°lise dos resultados.

---

**Solu√ß√£o 2 (Pool de Threads)**

Os testes da solu√ß√£o com **pool de threads** foram realizados na seguinte m√°quina:
- **Modelo:** Dell Inc. Inspiron 3501  
- **Mem√≥ria RAM:** 20,0 GiB  
- **Processador:** 11th Gen Intel¬Æ Core‚Ñ¢ i5-1135G7 (8 threads)  
- **Gr√°ficos:** Intel¬Æ Xe Graphics (TGL GT2) / NV138  
- **Disco:** 1 TB SSD  
- **Sistema Operacional:** Ubuntu 24.04.2 LTS (64 bits)  
- **Kernel:** Linux 6.8.0-87-generic  
- **Interface gr√°fica:** GNOME 46 (X11)  

A metodologia de teste foi exatamente a mesma usada para a solu√ß√£o fork:

1. O servidor foi iniciado em um terminal.  
2. Em outro terminal, o script `stress_test.sh` foi executado.  
3. O script dispara um n√∫mero vari√°vel de clientes (NUM_CLIENTS) em background, simulando requisi√ß√µes simult√¢neas.  
4. Os testes foram realizados com os seguintes valores: **100, 500, 1000, 2000, 4000, 8000, 12000, 50000 e 100000 clientes**.  
5. A dura√ß√£o total (tempo real) foi registrada com o comando `time`.  

**Resultados ‚Äì Solu√ß√£o 2 (Pool de Threads)**

| N√∫mero de clientes | Erros observados (cliente) | Erros observados (servidor) | Dura√ß√£o Total (real) |
| :--- | :--- | :--- | :--- |
| 100 | ‚Äì | ‚Äì | 0m0,020s |
| 500 | ‚Äì | ‚Äì | 0m0,101s |
| 1000 | ‚Äì | ‚Äì | 0m0,184s |
| 2000 | ‚Äì | ‚Äì | 0m0,431s |
| 4000 | ‚Äì | ‚Äì | 0m0,905s |
| 8000 | ‚Äì | ‚Äì | 0m2,222s |
| 12000 | ‚Äì | ‚Äì | 0m4,912s |
| 50000 | ‚Äì | ‚Äì | 1m34,053s |
| 100000 | ‚Äì | ‚Äì | 7m26,969s |

---
**üìà An√°lise dos Resultados**

A compara√ß√£o entre as duas solu√ß√µes evidencia diferen√ßas importantes:

**1. Desempenho**
- O pool de threads foi **drasticamente mais r√°pido** para cargas de at√© 12.000 clientes.  
- A solu√ß√£o fork apresentou tempos mais irregulares, com picos anormais (ex.: 1000 ‚Üí 2 minutos).  
- Para cargas muito altas (50.000 e 100.000 clientes), o desempenho das duas solu√ß√µes se aproxima, mas:
  - o thread pool ainda √© mais r√°pido at√© ~50.000 clientes,
  - e possui comportamento mais est√°vel.

**2. Estabilidade**
- A solu√ß√£o fork apresentou **erros de ‚ÄúConnection reset by peer‚Äù** nas cargas de 1000 e 2000 clientes.
- A solu√ß√£o com threads **n√£o apresentou erros em nenhum teste**, demonstrando:
  - menor overhead,  
  - maior consist√™ncia,  
  - melhor capacidade de processar conex√µes em alta frequ√™ncia.

**3. Uso de recursos**
- A cria√ß√£o de processos tem custos adicionais:
  - duplica√ß√£o de descritores,
  - tabelas de p√°gina,
  - mudan√ßa de contexto mais pesada.
- O pool de threads:
  - reutiliza threads,
  - reduz drasticamente overhead,
  - mant√©m consumo de mem√≥ria est√°vel.

**4. Escalabilidade**
- O modelo fork escalou, mas com instabilidade.
- O modelo com threads escalou de forma **linear e previs√≠vel at√© o limite de 100.000 clientes**.

---

**üèÅ Conclus√£o**

Os testes demonstram que a **solu√ß√£o baseada em pool de threads √© significativamente superior** √† solu√ß√£o baseada em fork para o cen√°rio do trabalho.  

Ela apresenta:

- tempos de resposta muito menores,
- comportamento est√°vel mesmo sob alta carga,
- aus√™ncia de erros de conex√£o,
- menor consumo de recursos,
- maior escalabilidade geral.

J√° o fork, apesar de funcional, demonstrou:

- custo elevado para cria√ß√£o de processos,
- instabilidade em cargas intermedi√°rias,
- maior variabilidade nos tempos de resposta.


A seguir est√° o gr√°fico utilizado na an√°lise dos resultados.  
A imagem foi gerada a partir dos dados coletados nos testes para as duas solu√ß√µes (fork e pool de threads):

![Compara√ß√£o entre fork e pool de threads](/resources/grafico-comparativo.png)

Assim, para sistemas que precisam lidar com dezenas de milhares de requisi√ß√µes simult√¢neas ‚Äî como servidores de compila√ß√£o, APIs paralelas ou servidores TCP concorrentes ‚Äî **o modelo com pool de threads deve ser preferido**, sendo mais leve, mais r√°pido e mais est√°vel.

---

### Quest√£o 3: Servidor de Compila√ß√£o e Cliente GUI

> Com a solu√ß√£o determinada no passo anterior, crie um servidor que efetue os servi√ßos abaixo: 

#### a. Servidor de Compila√ß√£o

O servidor implementado utiliza a abordagem de `[Solu√ß√£o escolhida na Quest√£o 2]` e oferece suporte para a compila√ß√£o e execu√ß√£o da linguagem Python.

* **C√≥digo-fonte:** compile-server/server.c
* **Protocolo de Comunica√ß√£o:** 

O protocolo de comunica√ß√£o entre o Cliente GUI (Python) e o Servidor de Compila√ß√£o (C) √© um protocolo simples baseado em texto/bin√°rio de requisi√ß√£o-resposta. Ele utiliza um cabe√ßalho de modo (Intention Header) para informar ao servidor a a√ß√£o desejada antes de enviar o c√≥digo fonte.

O processo de comunica√ß√£o segue o padr√£o TCP (Transmission Control Protocol) de sockets orientados a stream: o cliente conecta, envia todos os dados, o servidor processa e envia a resposta, e a conex√£o √© fechada.

#### b.Cliente com Interface Gr√°fica (GUI) 

Foi desenvolvido um cliente local para Linux com interface gr√°fica que permite ao usu√°rio interagir com o servidor de compila√ß√£o.

* **C√≥digo-fonte:** compile-server/main.py
* **Tecnologia/Biblioteca Gr√°fica:** foi utilizado o `Tkinter`
* **Funcionalidades Implementadas:**
    * [x] √Årea para edi√ß√£o de programa.
    * [x] √Årea para retornar os erros de compila√ß√£o.
    * [x] √Årea para retorno da sa√≠da do programa.
    * [x] Bot√£o para execu√ß√£o (enviar para o servidor).
    * [x] Bot√£o para baixar o programa compilado (desej√°vel).

#### Screenshot do Cliente

![alt text](resources/image.png)

---

## üõ†Ô∏è Como Compilar e Executar

### Pr√©-requisitos

`[Liste as depend√™ncias necess√°rias. Ex:
* gcc
* make
* Bibliotecas de desenvolvimento do GTK (libgtk-3-dev)
* Python 3.10 (para a GUI)]`

### 1. Servidor de Compila√ß√£o (Quest√£o 3a)

1. Compile o Servidor: Use o gcc conforme instru√≠do no texto.

Bash
```bash
gcc servidor_compiler.c -o servidor
```
2. Execute o Servidor: Escolha uma porta livre (ex: 51482).
Bash

```
./servidor 51482
```
O servidor ficar√° esperando conex√µes.

3. Execute o Cliente: Abra um novo terminal (ou use outra m√°quina na mesma rede, ajustando o IP no c√≥digo Python). Certifique-se de ter o python3 e o tkinter instalados (geralmente sudo apt-get install python3-tk).
Bash

```
python3 cliente_gui.py
```